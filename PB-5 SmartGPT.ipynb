{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Has chatGPT gotten dumber?\n",
    "<p style=\"margin-top: -20px; font-size: 0.8em;\">By Bryce Brady</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, there have been claims that GPT-4's reasoning abilities, coding skills, and overall performance have degraded over time or that OpenAI has \"nerfed\" the model. This was sparked in part by a paper from Stanford researchers (https://arxiv.org/pdf/2307.09009.pdf) claiming to show declines in GPT-4 performance over time but used a horendous methodology.\n",
    "\n",
    "As a regular user of GPT-4-0613 since its release, I have not personally noticed any changes. However, these are testable hypotheses, so I decided to conduct an analysis.\n",
    "\n",
    "In this notebook, I test three different versions of GPT-4 on a sample of problems from the MMLU Formal Logic Benchmark, with temperature set to 0 for all models:\n",
    "\n",
    "1. GPT-4 on May 8, 2023 (GPT-0314): I happened to perform this same analysis back on May 8, so we can compare the March vs June versions to see if changes were made, as some have claimed. The original analysis and version history is available here: [Smart GPT Eval](https://github.com/Bradybry/SmartGPT_eval)\n",
    "\n",
    "2. GPT-4-0314 today: I hypothesize the performance will be identical to May, contrary to claims that OpenAI has degraded the model over time.\n",
    "\n",
    "3. GPT-4-0613 today: I hypothesize the performance will be very similar to past versions.\n",
    "\n",
    "The benchmark dataset is available from Hugging Face here and a copy is included in ./data/ of this repository: [MMLU Formal Logic](https://huggingface.co/datasets/tasksource/mmlu) \n",
    "\n",
    "By testing multiple historic versions of GPT-4 on the same logic problems, we can see if performance has actually changed over time, or remained steady as I expect. This should provide evidence to evaluate claims from the Stanford paper and elsewhere that GPT-4 has been \"nerfed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and helper functions\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import tiktoken\n",
    "from sklearn.metrics import classification_report\n",
    "from expert import LanguageExpert\n",
    "tqdm.pandas()\n",
    "\n",
    "march_prompt = {'name': 'Test Taker',\n",
    "                'description': 'Correctly answer the following multiple choice question. Respond in the example_ouput format provided.',\n",
    "                'example_output': '<answer>{A, B, C, or D}</answer>',\n",
    "                'model_params' : {'model_name': 'gpt-4-0314', 'temperature': 0.0, 'frequency_penalty': 1.0, 'presence_penalty': 0.5, 'n': 1, 'max_tokens': 512}}\n",
    "june_prompt = {'name': 'Test Taker',\n",
    "                'description': 'Correctly answer the following multiple choice question. Respond in the example_ouput format provided.',\n",
    "                'example_output': '<answer>{A, B, C, or D}</answer>',\n",
    "                'model_params' : {'model_name': 'gpt-4-0613', 'temperature': 0.0, 'frequency_penalty': 1.0, 'presence_penalty': 0.5, 'n': 1, 'max_tokens': 512}}\n",
    "\n",
    "\n",
    "def generate_questions(row):\n",
    "    question = row['Question']\n",
    "    choices = row['Choices']\n",
    "    letters = ['A', 'B', 'C', 'D']\n",
    "    question =  f'<question>{question}</question>\\n'\n",
    "    choices = [f'\\n<choice_{letter}>{choice}</choice_{letter}>' for letter, choice in zip(letters, choices)]\n",
    "    for choice in choices:\n",
    "        question += choice\n",
    "    return question\n",
    "\n",
    "import re\n",
    "def extract_answer(xml_string):\n",
    "    # The re.DOTALL flag allows the dot to match newline characters.\n",
    "    answer_pattern = re.compile(r'<answer>(.*?)<\\/answer>', re.IGNORECASE | re.DOTALL)\n",
    "    match = answer_pattern.search(xml_string)\n",
    "\n",
    "    if match:\n",
    "        # Extract the text content, strip any leading/trailing whitespace or newlines, and convert to lowercase.\n",
    "        answer_text = match.group(1).strip().lower()\n",
    "        return answer_text\n",
    "    return None\n",
    "\n",
    "\n",
    "target_names = {0: \"a\", 1: \"b\", 2: \"c\", 3: \"d\"}\n",
    "target_numbers = {v: k for k, v in target_names.items()}\n",
    "\n",
    "\n",
    "## We will estimate tokens using the OpenAI Ada encoding. Not perfect but probably good enough.\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "\n",
    "\n",
    "import ast\n",
    "# Define a function to convert string to list\n",
    "def str_to_list(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "def extract_digit(s):\n",
    "    match = re.search(r'\\d', s)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Choices</th>\n",
       "      <th>Answer</th>\n",
       "      <th>formatted_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Identify the conclusion of the following argum...</td>\n",
       "      <td>[It is hard not to verify in our peers the sam...</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;question&gt;Identify the conclusion of the follo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Select the best translation into predicate log...</td>\n",
       "      <td>[Tdc, Tcd, Tcc, dTc]</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;question&gt;Select the best translation into pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Select the best English interpretation of the ...</td>\n",
       "      <td>[Some large houses are bigger than some apartm...</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;question&gt;Select the best English interpretati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Construct a complete truth table for the follo...</td>\n",
       "      <td>[Valid, Invalid. Counterexample when G and H a...</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;question&gt;Construct a complete truth table for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Use the following key to translate the given f...</td>\n",
       "      <td>[If it's not the case that both Izzy plays Min...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;question&gt;Use the following key to translate t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Identify the conclusion of the following argum...   \n",
       "1  Select the best translation into predicate log...   \n",
       "2  Select the best English interpretation of the ...   \n",
       "3  Construct a complete truth table for the follo...   \n",
       "4  Use the following key to translate the given f...   \n",
       "\n",
       "                                             Choices  Answer  \\\n",
       "0  [It is hard not to verify in our peers the sam...       3   \n",
       "1                               [Tdc, Tcd, Tcc, dTc]       0   \n",
       "2  [Some large houses are bigger than some apartm...       2   \n",
       "3  [Valid, Invalid. Counterexample when G and H a...       0   \n",
       "4  [If it's not the case that both Izzy plays Min...       1   \n",
       "\n",
       "                                  formatted_question  \n",
       "0  <question>Identify the conclusion of the follo...  \n",
       "1  <question>Select the best translation into pre...  \n",
       "2  <question>Select the best English interpretati...  \n",
       "3  <question>Construct a complete truth table for...  \n",
       "4  <question>Use the following key to translate t...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load and format data\n",
    "file = './data/MMLUFL_HF_100.csv'\n",
    "df = pd.read_csv(file)\n",
    "df['Choices'] = df['Choices'].apply(str_to_list)\n",
    "df['Answer'] = df['Answer'].apply(extract_digit)\n",
    "df['Question'] = df['Question'].str.strip('\"').str.strip()\n",
    "df['formatted_question'] = df.apply(generate_questions, axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(25, random_state=42) # Used the same random state as the original test set from May"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## May 8th GPT-4 Test Results\n",
    "\n",
    "See original analysis for commit history of results. [Smart GPT Eval](https://github.com/Bradybry/SmartGPT_eval)\n",
    "\n",
    "Called the null_prompt in the original analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_stats = {'precision': 0.8044675324675324, 'recall': 0.68, 'f1': 0.6474920634920635, 'support': 25, 'completion_tokens': 6.0, 'prompt_tokens': 204.84, 'latency': 2.67309814453125}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4-0314 Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! frequency_penalty is not default parameter.\n",
      "                    frequency_penalty was transferred to model_kwargs.\n",
      "                    Please confirm that frequency_penalty is what you intended.\n",
      "WARNING! presence_penalty is not default parameter.\n",
      "                    presence_penalty was transferred to model_kwargs.\n",
      "                    Please confirm that presence_penalty is what you intended.\n",
      "100%|██████████| 25/25 [00:29<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "## Generate March Prompt Results\n",
    "march_expert = LanguageExpert(**march_prompt)\n",
    "\n",
    "start_time = time.time()\n",
    "df['march_result'] = df.progress_apply(lambda x: march_expert(x['formatted_question']), axis=1)\n",
    "time_per_it = (time.time() - start_time)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.6699047619047619, 'recall': 0.64, 'f1': 0.6059994907053731, 'support': 25, 'completion_tokens': 6.0, 'prompt_tokens': 204.84, 'latency': 1.1876980113983153}\n"
     ]
    }
   ],
   "source": [
    "## Pull out answers\n",
    "## TODO - this is a hacky way to do this. Should be a better way.\n",
    "df['march_answer'] = df.apply(lambda x: extract_answer(x['march_result']), axis=1)\n",
    "df['march_pred'] = df.march_answer.map(target_numbers)\n",
    "\n",
    "## Get token counts\n",
    "df[\"march_completion_tokens\"] = df.march_result.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "expert_tokens = len(encoding.encode(march_expert.get_content().content))\n",
    "df[\"march_prompt_tokens\"] = df.formatted_question.apply(lambda x: len(encoding.encode(x))) + expert_tokens\n",
    "\n",
    "## Calculate performance stats\n",
    "y_true = df['Answer']\n",
    "y_pred = df['march_pred']\n",
    "report = classification_report(y_true, y_pred,target_names=target_names.values(), labels=list(target_names.keys()), zero_division=1, output_dict=True)\n",
    "\n",
    "### Collect statistics\n",
    "march_stats = {\n",
    "    \"precision\": report['weighted avg']['precision'],\n",
    "    \"recall\": report['weighted avg']['recall'],\n",
    "    \"f1\":  report['weighted avg']['f1-score'],\n",
    "    \"support\": report['weighted avg']['support'],\n",
    "    \"completion_tokens\": df[\"march_completion_tokens\"].mean(),\n",
    "    \"prompt_tokens\": df[\"march_prompt_tokens\"].mean(),\n",
    "    \"latency\": time_per_it\n",
    "}\n",
    "\n",
    "print(march_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4-0614 Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! frequency_penalty is not default parameter.\n",
      "                    frequency_penalty was transferred to model_kwargs.\n",
      "                    Please confirm that frequency_penalty is what you intended.\n",
      "WARNING! presence_penalty is not default parameter.\n",
      "                    presence_penalty was transferred to model_kwargs.\n",
      "                    Please confirm that presence_penalty is what you intended.\n",
      "100%|██████████| 25/25 [00:29<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "## Generate June Prompt Results\n",
    "june_expert = LanguageExpert(**june_prompt)\n",
    "\n",
    "start_time = time.time()\n",
    "df['june_result'] = df.progress_apply(lambda x: june_expert(x['formatted_question']), axis=1)\n",
    "time_per_it = (time.time() - start_time)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.711, 'recall': 0.68, 'f1': 0.676923076923077, 'support': 25, 'completion_tokens': 6.0, 'prompt_tokens': 204.84, 'latency': 1.1905951309204101}\n"
     ]
    }
   ],
   "source": [
    "## Pull out answers\n",
    "## TODO - this is a hacky way to do this. Should be a better way.\n",
    "df['june_answer'] = df.apply(lambda x: extract_answer(x['june_result']), axis=1)\n",
    "df['june_pred'] = df.june_answer.map(target_numbers)\n",
    "\n",
    "## Get token counts\n",
    "df[\"june_completion_tokens\"] = df.june_result.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "expert_tokens = len(encoding.encode(june_expert.get_content().content))\n",
    "df[\"june_prompt_tokens\"] = df.formatted_question.apply(lambda x: len(encoding.encode(x))) + expert_tokens\n",
    "\n",
    "## Calculate performance stats\n",
    "y_true = df['Answer']\n",
    "y_pred = df['june_pred']\n",
    "report = classification_report(y_true, y_pred,target_names=target_names.values(), labels=list(target_names.keys()), zero_division=1, output_dict=True)\n",
    "\n",
    "### Collect statistics\n",
    "june_stats = {\n",
    "    \"precision\": report['weighted avg']['precision'],\n",
    "    \"recall\": report['weighted avg']['recall'],\n",
    "    \"f1\":  report['weighted avg']['f1-score'],\n",
    "    \"support\": report['weighted avg']['support'],\n",
    "    \"completion_tokens\": df[\"june_completion_tokens\"].mean(),\n",
    "    \"prompt_tokens\": df[\"june_prompt_tokens\"].mean(),\n",
    "    \"latency\": time_per_it\n",
    "}\n",
    "\n",
    "print(june_stats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>May GPT-4</th>\n",
       "      <td>0.804468</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.647492</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>204.84</td>\n",
       "      <td>2.673098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-0314</th>\n",
       "      <td>0.669905</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.605999</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>204.84</td>\n",
       "      <td>1.187698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-0613</th>\n",
       "      <td>0.711000</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>204.84</td>\n",
       "      <td>1.190595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's display a table of the stats before doing any analysis.\n",
    "from IPython.display import display, HTML, Markdown\n",
    "data = {\"May GPT-4\": may_stats, \"gpt-4-0314\": march_stats,\"gpt-4-0613\": june_stats}\n",
    "results = pd.DataFrame(data).transpose()\n",
    "\n",
    "# Display the HTML table\n",
    "display(HTML(results.to_html()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "I set out with two hypotheses: \n",
    "\n",
    "1. The May GPT-4 and GPT-0314 (today) would have identical performance\n",
    "2. GPT-4-0613 would have slightly different but overall equal or better performance.\n",
    "\n",
    "The first hypothesis was clearly wrong based on the results. GPT-4-0314 performed significantly worse on this benchmark compared to May GPT-4, indicating changes have been made to the model over time.\n",
    "\n",
    "The second hypothesis was supported - GPT-4-0613 did achieve better performance than both past versions on the formal logic problems. However, this is just a single benchmark with a limited set of examples. We can't extend the conclusion too broadly given the small sample size.\n",
    "\n",
    "OpenAI is making changes to the model overtime that are causing variations in performance. This analysis shows that they seem to be getting better at formal reasoning, but perhaps it is getting worse in other areas. There may be some veracity to the claims that OpenAI is nerfing the model, but this analysis shows little proof of that claim. In fact, the current latest model is the most performant on the test set used in this analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
